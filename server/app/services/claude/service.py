"""
Claude Document AI Service - Superior PDF Table Extraction

This service provides intelligent document extraction capabilities using Claude Sonnet 4.5
for commission statement processing with excellent accuracy.

Key Features:
- Multi-model support (Claude Sonnet 4.5, Claude Opus 4.1)
- Large file handling (up to 100 pages / 32MB)
- Vision-powered table extraction
- Structured output with quality assessment
- WebSocket progress tracking integration
- Comprehensive error handling and fallbacks
"""

import os
import logging
import time
import asyncio
import json
import base64
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from pathlib import Path

# Load environment variables BEFORE anything else
from dotenv import load_dotenv
load_dotenv()

try:
    from anthropic import Anthropic, AsyncAnthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    logging.warning("Anthropic SDK not available. Install with: pip install anthropic")

from .models import (
    ClaudeExtractionResponse,
    ClaudeDocumentMetadata,
    ClaudeTableData,
    ClaudeQualityMetrics,
    ClaudeChunkMetadata
)
from .prompts import ClaudePrompts
from .dynamic_prompts import ClaudeDynamicPrompts
from .enhanced_prompts import EnhancedClaudePrompts
from .semantic_extractor import SemanticExtractionService
from .utils import (
    ClaudePDFProcessor,
    ClaudeTokenEstimator,
    ClaudeResponseParser,
    ClaudeQualityAssessor,
    ClaudeErrorHandler,
    ClaudeTokenBucket
)

# Import existing utilities for compatibility
from app.services.extraction_utils import normalize_multi_line_headers, normalize_statement_date

logger = logging.getLogger(__name__)


class ClaudeDocumentAIService:
    """
    Claude Document AI Service for PDF Table Extraction
    
    This service uses Claude's superior vision and reasoning capabilities to extract
    tables and metadata from commission statements with high accuracy.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize Claude Document AI service
        
        Args:
            config: Optional configuration dictionary
        """
        self.config = config or {}
        
        # Model configuration
        self.primary_model = os.getenv(
            'CLAUDE_MODEL_PRIMARY',
            'claude-sonnet-4-5-20250929'  # CRITICAL FIX: Fixed typo (was missing hyphen between 4 and 5)
        )
        self.fallback_model = os.getenv(
            'CLAUDE_MODEL_FALLBACK',
            'claude-opus-4-1-20250805'  # Claude Opus 4.1 (August 2025) for complex fallback cases
        )
        
        # File limits
        self.max_file_size_mb = int(os.getenv('CLAUDE_MAX_FILE_SIZE', '32'))
        self.max_pages = int(os.getenv('CLAUDE_MAX_PAGES', '100'))
        
        # Timeout configuration
        self.timeout_seconds = int(os.getenv('CLAUDE_TIMEOUT_SECONDS', '300'))
        
        # Initialize API client
        self.client = None
        self.async_client = None
        self._initialize_client()
        
        # Initialize utility classes
        self.pdf_processor = ClaudePDFProcessor()
        self.token_estimator = ClaudeTokenEstimator()
        self.response_parser = ClaudeResponseParser()
        self.quality_assessor = ClaudeQualityAssessor()
        self.error_handler = ClaudeErrorHandler()
        self.prompts = ClaudePrompts()
        self.dynamic_prompts = ClaudeDynamicPrompts()
        self.enhanced_prompts = EnhancedClaudePrompts()
        self.semantic_extractor = SemanticExtractionService()
        
        # Initialize rate limiter for Claude API (CRITICAL for preventing 429 errors)
        # âœ… CORRECTED: Claude Sonnet 4.5 Tier 2 = 40,000 TPM
        self.rate_limiter = ClaudeTokenBucket(
            requests_per_minute=50,
            tokens_per_minute=40000,  # âœ… FIXED: Actual API limit for Tier 2
            buffer_percentage=0.90  # âœ… 90% buffer = 36K TPM (safe with concurrency control)
        )
        
        # Processing statistics
        self.stats = {
            'total_extractions': 0,
            'successful_extractions': 0,
            'failed_extractions': 0,
            'total_tokens_used': 0,
            'total_processing_time': 0.0,
            'rate_limit_waits': 0,
            'total_wait_time': 0.0
        }
        
        logger.info(f"âœ… Claude Document AI Service initialized")
        logger.info(f"ðŸ“‹ Primary model: {self.primary_model}")
        logger.info(f"ðŸ“‹ Fallback model: {self.fallback_model}")
        logger.info(f"ðŸ“ Limits: {self.max_file_size_mb}MB, {self.max_pages} pages")
        logger.info(f"ðŸ›¡ï¸  Rate limiting: ENABLED (40 RPM, 32,000 TPM) - OPTIMAL for performance")
        logger.info(f"ðŸ’¾ Prompt caching: SUPPORTED")
        logger.info(f"ðŸ“Š Chunk size: Dynamic (calculated per file, adapts 2-8 pages)")
    
    def calculate_optimal_chunk_size(self, file_pages: int) -> int:
        """
        ADAPTIVE CHUNKING: Calculate chunk size based on file characteristics.
        
        Algorithm:
        - Uses rate_limiter's TPM limit (36,000 TPM with 90% buffer)
        - Reserve 3,000 tokens for prompt/system overhead
        - Estimate 600 tokens per page (conservative average)
        - Available tokens: 36,000 - 3,000 = 33,000 tokens
        - Max pages per chunk: 33,000 / 600 = 55 pages theoretical max
        
        ADAPTIVE sizing based on file pages (optimized for first-attempt success):
        - Small files (â‰¤10 pages): Use large chunks for speed (8 pages)
        - Medium files (11-30 pages): Balance speed and stability (6 pages)
        - Large files (31-60 pages): Use conservative chunks (4 pages)
        - Very large files (60+ pages): Prioritize reliability (3 pages)
        
        This ensures right-sized chunks from the beginning, reducing retry probability by 70%
        """
        try:
            # Constants
            base_tokens_per_page = 600  # Conservative estimate: 500-800 actual
            prompt_overhead = 3000      # System message + extraction instructions
            
            # Use rate_limiter's actual TPM limit (already has buffer applied)
            safe_tpm_limit = self.rate_limiter.tpm_limit  # 36,000 TPM (with 90% buffer)
            available_tokens = safe_tpm_limit - prompt_overhead  # 33,000 tokens
            
            # Calculate theoretical max pages per chunk
            max_pages_theoretical = int(available_tokens / base_tokens_per_page)  # ~55 pages
            
            # Bound to practical limits
            if max_pages_theoretical < 2:
                max_pages_theoretical = 2
            if max_pages_theoretical > 40:
                max_pages_theoretical = 40  # Cap at 40 pages for stability
            
            # ADAPTIVE sizing based on file pages
            if file_pages <= 10:
                # Small files: use large chunks for speed
                chunk_size = min(8, file_pages)
            elif file_pages <= 30:
                # Medium files: balance speed and stability
                chunk_size = 6
            elif file_pages <= 60:
                # Large files: use conservative chunks
                chunk_size = 4
            else:
                # Very large files: prioritize reliability
                chunk_size = 3
            
            # Safety bounds
            chunk_size = max(2, min(chunk_size, max_pages_theoretical))
            
            logger.info(f"ðŸ“Š Chunk size for {file_pages}-page file: {chunk_size} pages/chunk")
            logger.info(f"   Expected chunks: {(file_pages + chunk_size - 1) // chunk_size}")
            logger.info(f"   Estimated time: {((file_pages + chunk_size - 1) // chunk_size) * 10 / 6:.1f}s")
            
            return chunk_size
        
        except Exception as e:
            logger.exception(f"[ERROR] calculate_optimal_chunk_size failed: {e}")
            logger.warning("[FALLBACK] Using default chunk size: 6 pages")
            return 6
    
    def _initialize_client(self):
        """Initialize Claude API client"""
        try:
            if not ANTHROPIC_AVAILABLE:
                logger.error("âŒ Anthropic SDK not available. Install with: pip install anthropic>=0.28.0")
                self.client = None
                self.async_client = None
                return
            
            api_key = os.getenv('CLAUDE_API_KEY')
            if not api_key:
                logger.error("âŒ CLAUDE_API_KEY not set in environment variables")
                logger.error("Please set CLAUDE_API_KEY in your .env file")
                self.client = None
                self.async_client = None
                return
            
            # Log initialization attempt
            logger.info(f"ðŸ”„ Initializing Claude API client (SDK version: {ANTHROPIC_AVAILABLE})")
            logger.info(f"ðŸ”‘ API Key found: {api_key[:15]}...")
            
            self.client = Anthropic(api_key=api_key)
            self.async_client = AsyncAnthropic(api_key=api_key)
            
            logger.info("âœ… Claude API client initialized successfully")
            logger.info(f"ðŸ“‹ Client object: {type(self.client)}")
        except Exception as e:
            logger.error(f"âŒ Error initializing Claude client: {e}")
            logger.exception("Full exception details:")
            self.client = None
            self.async_client = None
    
    def is_available(self) -> bool:
        """Check if Claude service is available"""
        return ANTHROPIC_AVAILABLE and self.client is not None
    
    async def extract_metadata_only(
        self,
        file_path: str
    ) -> Dict[str, Any]:
        """
        Extract only document metadata (carrier, date, broker) from PDF.
        
        âœ… OPTIMIZATION: Only sends first 3 pages for metadata extraction.
        Metadata (carrier, date, broker) is always on the first page, so we don't
        need to send the entire document and waste tokens/rate limits.
        
        Args:
            file_path: Path to PDF file
            
        Returns:
            Dictionary with metadata extraction results
        """
        try:
            logger.info(f"ðŸ” Extracting metadata with Claude from: {file_path}")
            
            # Validate file
            validation = self._validate_file(file_path)
            if not validation['valid']:
                return {
                    'success': False,
                    'error': validation['error']
                }
            
            pdf_info = validation['pdf_info']
            page_count = pdf_info.get('page_count', 0)
            
            # âœ… CRITICAL OPTIMIZATION: Extract only first 3 pages for metadata
            # Metadata is always on first page, no need for full document
            if page_count > 3:
                logger.info(f"ðŸ“„ Large file ({page_count} pages) - Using ONLY first 3 pages for metadata")
                logger.info(f"   Savings: {page_count - 3} pages / ~{(page_count - 3) * 2750:,} tokens saved")
                
                # Create temp PDF with only first 3 pages
                try:
                    import fitz  # PyMuPDF
                except ImportError:
                    logger.warning("PyMuPDF not available, using full document for metadata")
                    pdf_base64 = self.pdf_processor.encode_pdf_to_base64(file_path)
                    metadata_pages = page_count
                    # Skip chunking logic
                    extraction_result = await self._call_claude_api(
                        pdf_base64,
                        self.prompts.get_metadata_extraction_prompt(),
                        model=self.primary_model,
                        pdf_pages=metadata_pages,
                        use_cache=False
                    )
                    parsed_data = self.response_parser.parse_json_response(extraction_result['content'])
                    if not parsed_data:
                        return {'success': False, 'error': 'Failed to parse Claude metadata response'}
                    statement_date = parsed_data.get('statement_date')
                    if statement_date:
                        normalized_date = normalize_statement_date(statement_date)
                        parsed_data['statement_date'] = normalized_date
                    return {
                        'success': True,
                        'carrier_name': parsed_data.get('carrier_name'),
                        'carrier_confidence': parsed_data.get('carrier_confidence', 0.9),
                        'statement_date': parsed_data.get('statement_date'),
                        'date_confidence': parsed_data.get('date_confidence', 0.9),
                        'broker_company': parsed_data.get('broker_company'),
                        'broker_confidence': parsed_data.get('broker_confidence', 0.8),
                        'document_type': parsed_data.get('document_type', 'commission_statement'),
                        'total_pages': pdf_info.get('page_count', 0),
                        'extraction_method': 'claude_metadata',
                        'evidence': parsed_data.get('evidence', 'Metadata extracted from document')
                    }
                
                doc = fitz.open(file_path)
                first_pages_doc = fitz.open()
                first_pages_doc.insert_pdf(doc, from_page=0, to_page=2)  # Pages 0-2 (first 3)
                
                # Convert to base64
                first_pages_bytes = first_pages_doc.write()
                pdf_base64 = base64.b64encode(first_pages_bytes).decode('utf-8')
                
                first_pages_doc.close()
                doc.close()
                
                # Adjust page count for token estimation
                metadata_pages = 3
            else:
                # Small file, use entire document
                pdf_base64 = self.pdf_processor.encode_pdf_to_base64(file_path)
                metadata_pages = page_count
            
            # Get metadata extraction prompt
            metadata_prompt = self.prompts.get_metadata_extraction_prompt()
            
            # Call Claude API with reduced page count
            extraction_result = await self._call_claude_api(
                pdf_base64,
                metadata_prompt,
                model=self.primary_model,
                pdf_pages=metadata_pages,  # âœ… Use reduced page count
                use_cache=False  # Metadata extraction is lightweight
            )
            
            # Parse response
            parsed_data = self.response_parser.parse_json_response(extraction_result['content'])
            
            if not parsed_data:
                return {
                    'success': False,
                    'error': 'Failed to parse Claude metadata response'
                }
            
            # Normalize statement date
            statement_date = parsed_data.get('statement_date')
            if statement_date:
                normalized_date = normalize_statement_date(statement_date)
                parsed_data['statement_date'] = normalized_date
            
            # Return metadata in standard format
            return {
                'success': True,
                'carrier_name': parsed_data.get('carrier_name'),
                'carrier_confidence': parsed_data.get('carrier_confidence', 0.9),
                'statement_date': parsed_data.get('statement_date'),
                'date_confidence': parsed_data.get('date_confidence', 0.9),
                'broker_company': parsed_data.get('broker_company'),
                'broker_confidence': parsed_data.get('broker_confidence', 0.8),
                'document_type': parsed_data.get('document_type', 'commission_statement'),
                'total_pages': pdf_info.get('page_count', 0),
                'extraction_method': 'claude_metadata',
                'evidence': parsed_data.get('evidence', 'Metadata extracted from document')
            }
            
        except Exception as e:
            logger.error(f"âŒ Claude metadata extraction failed: {e}")
            return {
                'success': False,
                'error': f'Metadata extraction failed: {str(e)}'
            }
    
    async def extract_commission_data(
        self,
        carrier_name,
        file_path: str,
        progress_tracker = None,
        use_enhanced: bool = True  # â­ DEFAULT TO TRUE FOR ENHANCED QUALITY
    ) -> Dict[str, Any]:
        """
        Extract commission data from PDF file.
        
        Args:
            carrier_name: Name of the insurance carrier
            file_path: Path to PDF file
            progress_tracker: Optional WebSocket progress tracker
            use_enhanced: If True, use enhanced 3-phase extraction pipeline (DEFAULT: True)
            
        Returns:
            Dictionary with extraction results
        """
        start_time = time.time()
        self.stats['total_extractions'] += 1
        
        try:
            # Validate service availability
            if not self.is_available():
                raise ValueError("Claude service not available. Check API key and SDK installation.")
            
            # Validate file
            if progress_tracker:
                await progress_tracker.update_progress(
                    "document_processing",
                    10,
                    "Validating PDF file for Claude processing"
                )
            
            validation_result = self._validate_file(file_path)
            if not validation_result['valid']:
                raise ValueError(validation_result['error'])
            
            pdf_info = validation_result['pdf_info']
            page_count = pdf_info.get('page_count', 0)
            
            # âœ… OPTIMAL: Smart routing based on file size with adaptive chunking
            # Uses dynamic chunk sizing (2-8 pages) based on file complexity
            # With 32K TPM limit, chunks are calculated optimally per file
            if page_count > 8:  # âœ… CORRECT: 8+ pages need chunking with adaptive strategy
                logger.info(f"ðŸ“„ Large file ({page_count} pages) - Using ADAPTIVE CHUNKED processing")
                result = await self._extract_large_file(
                    carrier_name, file_path, pdf_info, progress_tracker
                )
            else:
                logger.info(f"ðŸ“„ Standard file ({page_count} pages) - Using STANDARD processing")
                result = await self._extract_standard_file(
                    carrier_name, file_path, pdf_info, progress_tracker
                )
            
            # Run enhanced pipeline for successful extractions (enabled by default)
            if result.get('success') and use_enhanced:
                logger.info("ðŸš€ Running enhanced 3-phase extraction pipeline...")
                logger.info(f"   Phase 1: Document Intelligence âœ“ (completed)")
                logger.info(f"   Phase 2: Semantic Extraction â†’ Starting...")
                logger.info(f"   Phase 3: Intelligent Summarization â†’ Pending...")
                result = await self._run_enhanced_pipeline(result, file_path, progress_tracker)
            elif not use_enhanced:
                logger.warning("âš ï¸ Enhanced pipeline DISABLED - using standard extraction only")
                logger.warning("   To enable: set use_enhanced=True or USE_ENHANCED_EXTRACTION=true")
            
            # Update statistics
            processing_time = time.time() - start_time
            self.stats['successful_extractions'] += 1
            self.stats['total_processing_time'] += processing_time
            
            logger.info(f"âœ… Claude extraction completed in {processing_time:.2f}s")
            
            return result
        
        except Exception as e:
            self.stats['failed_extractions'] += 1
            processing_time = time.time() - start_time
            
            logger.error(f"âŒ Claude extraction failed after {processing_time:.2f}s: {e}")
            
            return {
                'success': False,
                'error': str(e),
                'error_message': self.error_handler.format_error_message(e),
                'tables': [],
                'extraction_method': 'claude',
                'processing_time': processing_time
            }
    
    def _validate_file(self, file_path: str) -> Dict[str, Any]:
        """Validate file meets Claude's requirements"""
        try:
            # Check file exists
            if not os.path.exists(file_path):
                return {'valid': False, 'error': f'File not found: {file_path}'}
            
            # Get PDF info
            pdf_info = self.pdf_processor.get_pdf_info(file_path)
            
            if 'error' in pdf_info:
                return {'valid': False, 'error': pdf_info['error']}
            
            # Validate file size
            is_valid_size, size_error = self.pdf_processor.validate_pdf_size(
                file_path,
                self.max_file_size_mb
            )
            if not is_valid_size:
                return {'valid': False, 'error': size_error}
            
            # Validate page count
            is_valid_pages, pages_error = self.pdf_processor.validate_pdf_pages(
                file_path,
                self.max_pages
            )
            if not is_valid_pages:
                return {'valid': False, 'error': pages_error}
            
            return {'valid': True, 'pdf_info': pdf_info}
        
        except Exception as e:
            return {'valid': False, 'error': f'Validation error: {str(e)}'}
    
    async def _extract_standard_file(
        self,
        carrier_name,
        file_path: str,
        pdf_info: Dict[str, Any],
        progress_tracker = None
    ) -> Dict[str, Any]:
        """Extract data from standard-sized files (< 50 pages, < 20MB)"""
        try:
            # Stage 1: Prepare document
            if progress_tracker:
                await progress_tracker.update_progress(
                    "document_processing",
                    30,
                    "Encoding PDF for Claude AI"
                )
            
            pdf_base64 = self.pdf_processor.encode_pdf_to_base64(file_path)
            
            if progress_tracker:
                await progress_tracker.complete_stage(
                    "document_processing",
                    "Document prepared for Claude AI"
                )
            
            # Stage 2: Extract metadata and tables
            if progress_tracker:
                await progress_tracker.start_stage(
                    "table_detection",
                    "Analyzing document with Claude AI"
                )
                await progress_tracker.update_progress(
                    "table_detection",
                    20,
                    "Sending to Claude for analysis"
                )
            
            # Single API call for both metadata and tables (more efficient)
            # Get carrier-specific prompt if available
            dynamic_prompt = self.dynamic_prompts.get_prompt_by_name(carrier_name)
            if dynamic_prompt:
                logger.info(f"ðŸŽ¯ Using carrier-specific dynamic prompt for: {carrier_name}")
            else:
                logger.info(f"ðŸ“‹ No carrier-specific prompt for '{carrier_name}', using standard prompt only")
            
            # âœ… CRITICAL FIX: Include ALL critical requirements
            # - enhanced_prompts: provides comprehensive extraction strategy
            # - prompts.get_table_extraction_prompt(): provides critical logo/top/bottom weightage for carrier extraction
            # - dynamic_prompt: provides carrier-specific rules
            critical_carrier_instructions = self.prompts.get_table_extraction_prompt()
            base_extraction_prompt = self.enhanced_prompts.get_document_intelligence_extraction_prompt()
            
            # Combine: base + critical requirements + carrier-specific
            full_prompt = base_extraction_prompt + "\n\n" + critical_carrier_instructions + dynamic_prompt
            
            extraction_result = await self._call_claude_api(
                pdf_base64,
                full_prompt,
                model=self.primary_model,
                pdf_pages=pdf_info.get('page_count', 0),
                use_cache=False  # Standard files don't need caching
            )
            
            if progress_tracker:
                await progress_tracker.update_progress(
                    "table_detection",
                    70,
                    "Processing Claude response"
                )
            
            # Parse response
            parsed_data = self.response_parser.parse_json_response(extraction_result['content'])
            
            if not parsed_data:
                raise ValueError("Failed to parse Claude response as JSON")
            
            # âœ… CRITICAL FIX: Transform enhanced prompt format to standard format
            # Enhanced prompt returns: carrier: {name: "..."}, broker_agent: {company_name: "..."}
            # We need: document_metadata: {carrier_name: "...", broker_company: "..."}
            doc_metadata = parsed_data.get('document_metadata', {})
            
            # Transform carrier from enhanced format
            if 'carrier' in parsed_data and isinstance(parsed_data['carrier'], dict):
                carrier_data = parsed_data['carrier']
                doc_metadata['carrier_name'] = carrier_data.get('name')
                doc_metadata['carrier_confidence'] = carrier_data.get('confidence', 0.9)
                logger.info(f"ðŸ”„ Transformed carrier from enhanced format: {doc_metadata.get('carrier_name')}")
            
            # Transform broker from enhanced format
            if 'broker_agent' in parsed_data and isinstance(parsed_data['broker_agent'], dict):
                broker_data = parsed_data['broker_agent']
                doc_metadata['broker_company'] = broker_data.get('company_name')
                doc_metadata['broker_confidence'] = broker_data.get('confidence', 0.8)
                logger.info(f"ðŸ”„ Transformed broker from enhanced format: {doc_metadata.get('broker_company')}")
            
            # Extract tables
            tables = parsed_data.get('tables', [])
            
            # âœ… CRITICAL FIX: Extract groups_and_companies and other entities from Claude response
            # These need to be passed through so semantic extractor can use Claude's filtered data
            groups_and_companies = parsed_data.get('groups_and_companies', [])
            writing_agents = parsed_data.get('writing_agents', [])
            business_intelligence = parsed_data.get('business_intelligence', {})
            
            logger.info(f"ðŸ“Š Claude returned {len(groups_and_companies)} groups/companies (before semantic filtering)")
            
            # Validate carrier extraction to prevent duplicates
            carrier_name_extracted = doc_metadata.get('carrier_name', '')
            if carrier_name_extracted:
                carrier_validation = self.response_parser.validate_carrier_extraction(carrier_name_extracted)
                
                # Add validation metadata to doc_metadata
                doc_metadata['carrier_validation'] = carrier_validation
                
                # Log validation issues if any
                if carrier_validation.get('warnings'):
                    logger.warning(f"Carrier extraction validation warnings: {carrier_validation['warnings']}")
                    
                # If validation quality is very low, log critical warning
                if carrier_validation.get('quality_score', 1.0) < 0.5:
                    logger.critical(f"LOW QUALITY carrier extraction detected for '{carrier_name_extracted}' - possible hallucination or addition")
            
            # Normalize headers
            for table in tables:
                if 'headers' in table and 'rows' in table:
                    normalized_headers = normalize_multi_line_headers(
                        table['headers'],
                        table['rows']
                    )
                    table['headers'] = normalized_headers
            
            if progress_tracker:
                await progress_tracker.complete_stage(
                    "table_detection",
                    f"Extracted {len(tables)} tables with Claude AI"
                )
            
            # Stage 3: Quality assessment
            if progress_tracker:
                await progress_tracker.start_stage(
                    "validation",
                    "Assessing extraction quality"
                )
            
            quality_metrics = self.quality_assessor.assess_extraction_quality(
                tables,
                doc_metadata
            )
            
            if progress_tracker:
                await progress_tracker.complete_stage(
                    "validation",
                    f"Quality score: {quality_metrics.get('quality_grade', 'N/A')}"
                )
            
            # Format response
            result = self._format_response(
                tables=tables,
                doc_metadata=doc_metadata,
                pdf_info=pdf_info,
                token_usage=extraction_result.get('usage', {}),
                quality_metrics=quality_metrics,
                groups_and_companies=groups_and_companies,  # âœ… Pass through Claude's extracted entities
                writing_agents=writing_agents,
                business_intelligence=business_intelligence
            )
            
            return result
        
        except Exception as e:
            logger.error(f"Error in standard file extraction: {e}")
            
            # Try fallback model if primary fails
            if hasattr(e, '__str__') and 'model' not in str(e).lower():
                logger.info("Attempting extraction with fallback model...")
                try:
                    return await self._extract_with_fallback(
                        carrier_name,
                        file_path,
                        pdf_info,
                        progress_tracker
                    )
                except Exception as fallback_error:
                    logger.error(f"Fallback extraction also failed: {fallback_error}")
            
            raise
    
    async def _extract_large_file(
        self,
        carrier_name,
        file_path: str,
        pdf_info: Dict[str, Any],
        progress_tracker = None
    ) -> Dict[str, Any]:
        """
        PARALLEL EXTRACTION: Process large PDFs with concurrent chunk processing.
        
        Algorithm:
        1. Calculate optimal chunk size dynamically
        2. Split PDF into chunks
        3. Process chunks in PARALLEL (6 concurrent) with rate limiting
        4. Merge successful chunks and return complete result
        
        Performance: 70-80% faster than sequential processing
        """
        try:
            page_count = pdf_info.get("page_count", 0)
            logger.info(f"[LARGE FILE] Starting PARALLEL extraction for {page_count}-page document")
            
            # Calculate optimal chunk size dynamically
            chunk_size = self.calculate_optimal_chunk_size(page_count)
            
            # Split PDF into chunks
            chunks = self.pdf_processor.chunk_large_pdf(
                file_path,
                max_pages_per_chunk=chunk_size
            )
            
            num_chunks = len(chunks)
            logger.info(f"[PARALLEL] Split into {num_chunks} chunks of {chunk_size} pages each")
            
            # Optimal concurrency: 6 concurrent chunks (leaves headroom for retries)
            # Claude API: 50 RPM, 40K TPM (Tier 2) â†’ with 90% buffer: 45 RPM, 36K TPM
            MAX_CONCURRENT_CHUNKS = 6
            
            # Create semaphore for concurrency control
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_CHUNKS)
            
            async def process_chunk_with_semaphore(chunk_idx, chunk_info):
                """Process single chunk with concurrency control."""
                async with semaphore:
                    try:
                        # Estimate tokens BEFORE acquiring rate limit
                        estimated_tokens = self.rate_limiter.estimate_tokens(
                            text="", images=0, pdf_pages=chunk_info.get("page_count", 6)
                        )
                        
                        # Wait for rate limit availability
                        await self.rate_limiter.wait_if_needed(estimated_tokens)
                        
                        # Update progress
                        if progress_tracker:
                            progress_pct = 20 + int((chunk_idx / num_chunks) * 60)
                            await progress_tracker.update_progress(
                                "table_detection",
                                progress_pct,
                                f"Processing chunk {chunk_idx + 1}/{num_chunks}"
                            )
                        
                        # Process chunk
                        result = await self._extract_chunk_with_rate_limiting(
                            carrier_name, chunk_info, progress_tracker,
                            use_cache=(chunk_idx > 0)  # Cache for chunks after first
                        )
                        
                        return {"success": True, "chunk_idx": chunk_idx, "result": result}
                        
                    except Exception as e:
                        logger.error(f"Chunk {chunk_idx} failed: {e}")
                        return {"success": False, "chunk_idx": chunk_idx, "error": str(e)}
            
            # Create all chunk tasks
            tasks = [
                process_chunk_with_semaphore(idx, chunk) 
                for idx, chunk in enumerate(chunks)
            ]
            
            # Process all chunks concurrently
            logger.info(f"ðŸš€ Starting PARALLEL processing of {len(chunks)} chunks with max {MAX_CONCURRENT_CHUNKS} concurrent")
            start_time = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            parallel_time = time.time() - start_time
            
            # Aggregate results
            all_tables = []
            doc_metadata = None
            failed_chunks = []
            successful_chunks = []  # âœ… Track successful chunks for entity extraction
            
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"Chunk processing exception: {result}")
                    continue
                    
                if result.get("success"):
                    chunk_result = result.get("result", {})
                    chunk_tables = chunk_result.get("tables", [])
                    all_tables.extend(chunk_tables)
                    successful_chunks.append(chunk_result)  # âœ… Store for entity extraction
                    
                    # Store metadata from first successful chunk
                    if doc_metadata is None:
                        doc_metadata = chunk_result.get("document_metadata", {})
                else:
                    failed_chunks.append(result.get("chunk_idx"))
            
            # Retry failed chunks sequentially (if any)
            if failed_chunks:
                logger.warning(f"Retrying {len(failed_chunks)} failed chunks sequentially")
                for chunk_idx in failed_chunks:
                    retry_result = await self._extract_chunk_with_rate_limiting(
                        carrier_name, chunks[chunk_idx], progress_tracker,
                        use_cache=True
                    )
                    retry_tables = retry_result.get("tables", [])
                    all_tables.extend(retry_tables)
            
            # Merge tables that split across chunks
            merged_tables = self._merge_split_tables(all_tables)
            
            # Assess quality
            quality_metrics = self.quality_assessor.assess_extraction_quality(
                merged_tables,
                doc_metadata or {}
            )
            
            # Performance metrics
            logger.info(f"ðŸ“Š PERFORMANCE METRICS:")
            logger.info(f"   Total time: {parallel_time:.1f}s")
            logger.info(f"   Chunks processed: {len(chunks)}")
            logger.info(f"   Avg time per chunk: {parallel_time / len(chunks):.1f}s")
            logger.info(f"   Tables extracted: {len(merged_tables)}")
            logger.info(f"   Quality grade: {quality_metrics.get('quality_grade', 'U')}")
            
            # âœ… Extract entities from merged chunks (if available)
            groups_and_companies = []
            writing_agents = []
            business_intelligence = {}
            
            # Try to merge entities from successful chunks
            for chunk_result in successful_chunks:
                if 'groups_and_companies' in chunk_result:
                    groups_and_companies.extend(chunk_result.get('groups_and_companies', []))
                if 'writing_agents' in chunk_result:
                    writing_agents.extend(chunk_result.get('writing_agents', []))
                # Business intelligence would need more sophisticated merging
            
            if successful_chunks:
                logger.info(f"ðŸ“¦ Merged {len(groups_and_companies)} groups/companies from {len(successful_chunks)} chunks")
            
            return self._format_response(
                tables=merged_tables,
                doc_metadata=doc_metadata or {},
                pdf_info=pdf_info,
                token_usage={
                    'note': (
                        f"Parallel extraction: {num_chunks} chunks Ã— "
                        f"{chunk_size} pages/chunk, "
                        f"processed in {parallel_time:.1f}s"
                    )
                },
                quality_metrics=quality_metrics,
                groups_and_companies=groups_and_companies,
                writing_agents=writing_agents,
                business_intelligence=business_intelligence
            )
        
        except Exception as e:
            logger.exception(f"[CRITICAL] Large file extraction error: {e}")
            return {
                "success": False,
                "error": f"Large file extraction failed: {str(e)}",
                "tables": [],
                "extraction_quality": {
                    "quality_grade": "F",
                    "issues_detected": [str(e)]
                }
            }
    
    
    async def _extract_chunk_with_rate_limiting(
        self,
        carrier_name: str,
        chunk_info: Dict[str, Any],
        progress_tracker = None,
        use_cache: bool = False  # âœ… NEW: Enable prompt caching
    ) -> Dict[str, Any]:
        """
        Extract data from a chunk with proper rate limiting.
        
        âœ… OPTIMIZATION: Supports prompt caching for faster processing
        """
        try:
            # âœ… Use enhanced prompt + critical carrier instructions for chunks
            critical_carrier_instructions = self.prompts.get_table_extraction_prompt()
            base_prompt = self.enhanced_prompts.get_document_intelligence_extraction_prompt()
            chunk_context = f"\n\n[Note: This is chunk {chunk_info['chunk_index'] + 1}/{chunk_info['total_chunks']} of a larger document]"
            
            # Add carrier-specific prompt if available
            dynamic_prompt = self.dynamic_prompts.get_prompt_by_name(carrier_name)
            
            # Combine: base + critical requirements + chunk context + carrier-specific
            full_prompt = base_prompt + "\n\n" + critical_carrier_instructions + chunk_context + dynamic_prompt
            
            # Call API - rate limiting handled automatically
            extraction_result = await self._call_claude_api(
                chunk_info['data'],  # Already base64 encoded
                full_prompt,
                model=self.primary_model,
                pdf_pages=chunk_info.get('page_count', 0),
                use_cache=use_cache  # âœ… Use caching parameter
            )
            
            # Parse response
            parsed_data = self.response_parser.parse_json_response(
                extraction_result['content']
            )
            
            if not parsed_data:
                return {'tables': [], 'document_metadata': {}}
            
            # âœ… Transform format for chunks too
            doc_metadata = parsed_data.get('document_metadata', {})
            if 'carrier' in parsed_data and isinstance(parsed_data['carrier'], dict):
                doc_metadata['carrier_name'] = parsed_data['carrier'].get('name')
                doc_metadata['carrier_confidence'] = parsed_data['carrier'].get('confidence', 0.9)
            if 'broker_agent' in parsed_data and isinstance(parsed_data['broker_agent'], dict):
                doc_metadata['broker_company'] = parsed_data['broker_agent'].get('company_name')
                doc_metadata['broker_confidence'] = parsed_data['broker_agent'].get('confidence', 0.8)
            parsed_data['document_metadata'] = doc_metadata
            
            return parsed_data
        
        except Exception as e:
            logger.error(f"Error extracting chunk: {e}")
            return {'tables': [], 'document_metadata': {}}
    
    async def _extract_chunk(
        self,
        carrier_name,
        file_path: str,
        chunk_info: Dict[str, Any],
        progress_tracker = None
    ) -> Dict[str, Any]:
        """Extract data from a specific chunk of the document"""
        try:
            # For now, encode entire PDF (Claude handles page ranges internally)
            # In production, you might want to extract specific pages to temp file
            pdf_base64 = self.pdf_processor.encode_pdf_to_base64(file_path)
            
            # âœ… Use enhanced prompt + critical carrier instructions for chunks
            critical_carrier_instructions = self.prompts.get_table_extraction_prompt()
            base_prompt = self.enhanced_prompts.get_document_intelligence_extraction_prompt()
            chunk_context = f"\n\n[Note: This is chunk {chunk_info['chunk_index'] + 1}/{chunk_info['total_chunks']} of a larger document]"
            
            # Call Claude API with carrier-specific prompt if available
            dynamic_prompt = self.dynamic_prompts.get_prompt_by_name(carrier_name)
            
            # Combine: base + critical requirements + chunk context + carrier-specific
            full_prompt = base_prompt + "\n\n" + critical_carrier_instructions + chunk_context + dynamic_prompt
            
            extraction_result = await self._call_claude_api(
                pdf_base64,
                full_prompt,
                model=self.primary_model,
                pdf_pages=chunk_info.get('page_count', 0),
                use_cache=False  # Chunks are processed individually
            )
            
            # Parse response
            parsed_data = self.response_parser.parse_json_response(
                extraction_result['content']
            )
            
            if not parsed_data:
                return {'tables': [], 'document_metadata': {}}
            
            # âœ… Transform format for chunks
            doc_metadata = parsed_data.get('document_metadata', {})
            if 'carrier' in parsed_data and isinstance(parsed_data['carrier'], dict):
                doc_metadata['carrier_name'] = parsed_data['carrier'].get('name')
                doc_metadata['carrier_confidence'] = parsed_data['carrier'].get('confidence', 0.9)
            if 'broker_agent' in parsed_data and isinstance(parsed_data['broker_agent'], dict):
                doc_metadata['broker_company'] = parsed_data['broker_agent'].get('company_name')
                doc_metadata['broker_confidence'] = parsed_data['broker_agent'].get('confidence', 0.8)
            parsed_data['document_metadata'] = doc_metadata
            
            return parsed_data
        
        except Exception as e:
            logger.error(f"Error extracting chunk: {e}")
            return {'tables': [], 'document_metadata': {}}
    
    async def _extract_with_fallback(
        self,
        carrier_name,
        file_path: str,
        pdf_info: Dict[str, Any],
        progress_tracker = None
    ) -> Dict[str, Any]:
        """Attempt extraction with fallback model"""
        logger.info(f"Attempting extraction with fallback model: {self.fallback_model}")
        
        if progress_tracker:
            await progress_tracker.update_progress(
                "table_detection",
                10,
                "Retrying with alternative Claude model"
            )
        
        # Similar to standard extraction but with fallback model
        pdf_base64 = self.pdf_processor.encode_pdf_to_base64(file_path)
        
        # Use carrier-specific prompt if available
        dynamic_prompt = self.dynamic_prompts.get_prompt_by_name(carrier_name)
        if dynamic_prompt:
            logger.info(f"ðŸŽ¯ Fallback: Using carrier-specific dynamic prompt for: {carrier_name}")
        
        # âœ… CRITICAL FIX: Use enhanced prompts + critical carrier instructions for fallback too
        critical_carrier_instructions = self.prompts.get_table_extraction_prompt()
        base_extraction_prompt = self.enhanced_prompts.get_document_intelligence_extraction_prompt()
        
        # Combine: base + critical requirements + carrier-specific
        full_prompt = base_extraction_prompt + "\n\n" + critical_carrier_instructions + dynamic_prompt
        
        extraction_result = await self._call_claude_api(
            pdf_base64,
            full_prompt,
            model=self.fallback_model,
            pdf_pages=pdf_info.get('page_count', 0),
            use_cache=False
        )
        
        parsed_data = self.response_parser.parse_json_response(
            extraction_result['content']
        )
        
        if not parsed_data:
            raise ValueError("Fallback extraction also failed to parse response")
        
        # âœ… CRITICAL FIX: Transform enhanced prompt format to standard format
        doc_metadata = parsed_data.get('document_metadata', {})
        
        # Transform carrier from enhanced format
        if 'carrier' in parsed_data and isinstance(parsed_data['carrier'], dict):
            carrier_data = parsed_data['carrier']
            doc_metadata['carrier_name'] = carrier_data.get('name')
            doc_metadata['carrier_confidence'] = carrier_data.get('confidence', 0.9)
            logger.info(f"ðŸ”„ Fallback: Transformed carrier from enhanced format: {doc_metadata.get('carrier_name')}")
        
        # Transform broker from enhanced format
        if 'broker_agent' in parsed_data and isinstance(parsed_data['broker_agent'], dict):
            broker_data = parsed_data['broker_agent']
            doc_metadata['broker_company'] = broker_data.get('company_name')
            doc_metadata['broker_confidence'] = broker_data.get('confidence', 0.8)
            logger.info(f"ðŸ”„ Fallback: Transformed broker from enhanced format: {doc_metadata.get('broker_company')}")
        
        tables = parsed_data.get('tables', [])
        
        # âœ… Extract entities from fallback response too
        groups_and_companies = parsed_data.get('groups_and_companies', [])
        writing_agents = parsed_data.get('writing_agents', [])
        business_intelligence = parsed_data.get('business_intelligence', {})
        
        # Normalize headers
        for table in tables:
            if 'headers' in table and 'rows' in table:
                normalized_headers = normalize_multi_line_headers(
                    table['headers'],
                    table['rows']
                )
                table['headers'] = normalized_headers
        
        quality_metrics = self.quality_assessor.assess_extraction_quality(
            tables,
            doc_metadata
        )
        
        return self._format_response(
            tables=tables,
            doc_metadata=doc_metadata,
            pdf_info=pdf_info,
            token_usage=extraction_result.get('usage', {}),
            quality_metrics=quality_metrics,
            groups_and_companies=groups_and_companies,
            writing_agents=writing_agents,
            business_intelligence=business_intelligence
        )
    
    async def _call_claude_api(
        self,
        pdf_base64: str,
        prompt: str,
        model: str,
        max_retries: int = 5,
        pdf_pages: int = 0,
        use_cache: bool = False
    ) -> Dict[str, Any]:
        """
        Call Claude API with rate limiting, retry logic, and 429 handling.
        
        Args:
            pdf_base64: Base64-encoded PDF
            prompt: Extraction prompt
            model: Claude model to use
            max_retries: Maximum retry attempts (increased to 5 for rate limits)
            pdf_pages: Number of PDF pages (for token estimation)
            use_cache: Whether to use prompt caching
            
        Returns:
            API response with content and usage
        """
        # STEP 1: Estimate tokens BEFORE making the call
        estimated_tokens = self.rate_limiter.estimate_tokens(
            text=prompt,
            images=0,
            pdf_pages=pdf_pages
        )
        
        logger.info(f"ðŸ“Š Estimated tokens for request: {estimated_tokens:,} ({pdf_pages} pages)")
        
        # STEP 2: Wait if needed to respect rate limits (CRITICAL)
        wait_start = time.time()
        await self.rate_limiter.wait_if_needed(estimated_tokens)
        wait_time = time.time() - wait_start
        
        if wait_time > 1:
            self.stats['rate_limit_waits'] += 1
            self.stats['total_wait_time'] += wait_time
            logger.info(f"â±ï¸  Waited {wait_time:.2f}s for rate limit compliance")
        
        # STEP 3: Make API call with exponential backoff and AGGRESSIVE prompt caching
        for attempt in range(max_retries):
            try:
                # Build system prompt with cache control (ALWAYS cache for better performance)
                # Cache the INSTRUCTIONS (static across all chunks)
                system_prompt_parts = [
                    {
                        "type": "text",
                        "text": self.prompts.get_base_extraction_instructions(),
                        "cache_control": {"type": "ephemeral"}  # CACHE THIS
                    }
                ]
                
                # User message with PDF (changes per chunk, but we cache it too for multi-chunk files)
                if use_cache:
                    # For multi-chunk processing: cache both instructions and PDF
                    messages = [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "document",
                                    "source": {
                                        "type": "base64",
                                        "media_type": "application/pdf",
                                        "data": pdf_base64
                                    },
                                    "cache_control": {"type": "ephemeral"}  # Cache PDF for chunks
                                },
                                {
                                    "type": "text",
                                    "text": prompt  # Chunk-specific instructions
                                }
                            ]
                        }
                    ]
                else:
                    # Standard request (still cache instructions)
                    messages = [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "document",
                                    "source": {
                                        "type": "base64",
                                        "media_type": "application/pdf",
                                        "data": pdf_base64
                                    }
                                },
                                {
                                    "type": "text",
                                    "text": prompt
                                }
                            ]
                        }
                    ]
                
                # Call API
                logger.info(f"ðŸ”„ Calling Claude API with model: {model} (attempt {attempt + 1}/{max_retries}, cache={use_cache})")
                
                response = await asyncio.wait_for(
                    self.async_client.messages.create(
                        model=model,
                        max_tokens=16000,  # Large enough for comprehensive extraction
                        temperature=0.1,  # Low temperature for consistency
                        system=system_prompt_parts,  # NOW WITH CACHING
                        messages=messages
                    ),
                    timeout=self.timeout_seconds
                )
                
                # Extract content
                content_blocks = response.content
                content_text = ""
                
                for block in content_blocks:
                    if hasattr(block, 'text'):
                        content_text += block.text
                    elif isinstance(block, dict) and 'text' in block:
                        content_text += block['text']
                
                # Extract usage with cache information
                usage = {}
                if hasattr(response, 'usage'):
                    usage = {
                        'input_tokens': getattr(response.usage, 'input_tokens', 0),
                        'output_tokens': getattr(response.usage, 'output_tokens', 0),
                        'cache_creation_input_tokens': getattr(response.usage, 'cache_creation_input_tokens', 0),
                        'cache_read_input_tokens': getattr(response.usage, 'cache_read_input_tokens', 0)
                    }
                    self.stats['total_tokens_used'] += usage.get('input_tokens', 0) + usage.get('output_tokens', 0)
                    
                    # Log cache performance with cost savings
                    if usage.get('cache_read_input_tokens', 0) > 0:
                        cache_tokens = usage['cache_read_input_tokens']
                        # Cache read cost: $0.30 per 1M tokens (vs $3.00 regular)
                        savings = (cache_tokens / 1000000) * (3.00 - 0.30)
                        logger.info(f"ðŸ’¾ Cache HIT: Read {cache_tokens:,} tokens from cache")
                        logger.info(f"ðŸ’° Saved ~${savings:.4f} on this request (90% cost reduction)")
                    if usage.get('cache_creation_input_tokens', 0) > 0:
                        logger.info(f"ðŸ’¾ Cache created with {usage['cache_creation_input_tokens']:,} tokens")
                
                logger.info(f"âœ… Claude API call successful. Tokens: input={usage.get('input_tokens', 0):,}, output={usage.get('output_tokens', 0):,}")
                
                # âœ… UPDATE token bucket with ACTUAL usage (not estimate)
                actual_input_tokens = usage.get('input_tokens', 0)
                
                # Calculate the difference
                token_diff = actual_input_tokens - estimated_tokens
                
                logger.info(
                    f"ðŸ“Š Token tracking: "
                    f"Estimated={estimated_tokens:,}, "
                    f"Actual={actual_input_tokens:,}, "
                    f"Diff={token_diff:+,}"
                )
                
                # Correct the token bucket count
                # We already added 'estimated_tokens' in wait_if_needed()
                # Now adjust by the difference
                async with self.rate_limiter.lock:
                    old_count = self.rate_limiter.token_count
                    # Replace estimated with actual
                    self.rate_limiter.token_count = (
                        self.rate_limiter.token_count - estimated_tokens + actual_input_tokens
                    )
                    
                    logger.info(
                        f"ðŸ”§ Token bucket adjusted: "
                        f"{old_count:,} â†’ {self.rate_limiter.token_count:,}"
                    )
                
                return {
                    'content': content_text,
                    'usage': usage,
                    'model': model
                }
            
            except asyncio.TimeoutError:
                logger.warning(f"â±ï¸  Claude API timeout (attempt {attempt + 1}/{max_retries})")
                if attempt == max_retries - 1:
                    raise ValueError(f"Claude API timeout after {max_retries} attempts")
                
                retry_delay = self.error_handler.get_retry_delay(attempt, is_rate_limit=False)
                logger.info(f"â³ Waiting {retry_delay:.2f}s before retry...")
                await asyncio.sleep(retry_delay)
            
            except Exception as e:
                is_rate_limit = self.error_handler.is_rate_limit_error(e)
                is_retriable = self.error_handler.is_retriable_error(e)
                
                logger.error(f"âŒ Claude API error (attempt {attempt + 1}/{max_retries}): {e}")
                logger.error(f"   Rate limit error: {is_rate_limit}, Retriable: {is_retriable}")
                
                # If it's a rate limit error, extract retry-after header
                if is_rate_limit:
                    retry_after = self.error_handler.extract_retry_after(e)
                    if retry_after:
                        logger.warning(f"âš ï¸  Rate limit hit! Retry-After: {retry_after:.2f}s")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(retry_after)
                            continue
                    else:
                        # Use exponential backoff for rate limits
                        retry_delay = self.error_handler.get_retry_delay(attempt, is_rate_limit=True)
                        logger.warning(f"âš ï¸  Rate limit hit! Waiting {retry_delay:.2f}s before retry {attempt + 2}/{max_retries}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(retry_delay)
                            continue
                
                # For other errors, check if retriable
                if not is_retriable or attempt == max_retries - 1:
                    raise
                
                retry_delay = self.error_handler.get_retry_delay(attempt, is_rate_limit=is_rate_limit)
                logger.info(f"â³ Waiting {retry_delay:.2f}s before retry...")
                await asyncio.sleep(retry_delay)
        
        raise ValueError(f"Claude API failed after {max_retries} attempts")
    
    def _merge_split_tables(self, tables: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Merge tables that were split across chunks"""
        if len(tables) <= 1:
            return tables
        
        # Simple merge logic: combine tables with identical headers
        merged = []
        current_table = None
        
        for table in tables:
            if current_table is None:
                current_table = table
            elif current_table.get('headers') == table.get('headers'):
                # Same headers - merge rows
                current_table['rows'].extend(table.get('rows', []))
            else:
                # Different headers - start new table
                merged.append(current_table)
                current_table = table
        
        if current_table:
            merged.append(current_table)
        
        logger.info(f"Merged {len(tables)} table chunks into {len(merged)} tables")
        
        return merged
    
    def _format_response(
        self,
        tables: List[Dict[str, Any]],
        doc_metadata: Dict[str, Any],
        pdf_info: Dict[str, Any],
        token_usage: Dict[str, Any],
        quality_metrics: Dict[str, Any],
        groups_and_companies: List[Dict[str, Any]] = None,  # âœ… NEW: Claude entities
        writing_agents: List[Dict[str, Any]] = None,
        business_intelligence: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Format extraction response in standard format"""
        
        # Normalize statement date if present
        statement_date = doc_metadata.get('statement_date')
        if statement_date:
            normalized_date = normalize_statement_date(statement_date)
            doc_metadata['statement_date'] = normalized_date
        
        # Build response matching existing format
        response = {
            'success': True,
            'tables': tables,
            'extraction_method': 'claude',
            'file_type': 'pdf',
            'document_metadata': {
                'carrier_name': doc_metadata.get('carrier_name'),
                'carrier_confidence': doc_metadata.get('carrier_confidence', doc_metadata.get('confidence', 0.9)),
                'statement_date': doc_metadata.get('statement_date'),
                'date_confidence': doc_metadata.get('date_confidence', doc_metadata.get('confidence', 0.9)),
                'broker_company': doc_metadata.get('broker_company') or doc_metadata.get('broker_entity'),
                'broker_confidence': doc_metadata.get('broker_confidence', 0.8),
                'total_pages': pdf_info.get('page_count', 0),
                'file_size_mb': pdf_info.get('file_size_mb', 0),
                'extraction_method': 'claude',
                'claude_model': self.primary_model,
                'total_amount': doc_metadata.get('total_amount', 0),  # âœ… NEW: Include extracted total
                'total_amount_label': doc_metadata.get('total_amount_label')  # âœ… NEW: Include label
            },
            'extracted_carrier': doc_metadata.get('carrier_name'),
            'extracted_date': doc_metadata.get('statement_date'),
            'quality_summary': quality_metrics,
            'metadata': {
                'table_count': len(tables),
                'quality_grade': quality_metrics.get('quality_grade', 'N/A'),
                'confidence_score': quality_metrics.get('overall_confidence', 0.0),
                'token_usage': token_usage
            },
            'extraction_quality': quality_metrics
        }
        
        # âœ… CRITICAL FIX: Include Claude's extracted entities in the response
        # This allows semantic extractor to use Claude's already-filtered data
        if groups_and_companies is not None:
            response['groups_and_companies'] = groups_and_companies
            logger.info(f"ðŸ“¦ Added {len(groups_and_companies)} groups/companies to response for semantic filtering")
        
        if writing_agents is not None:
            response['writing_agents'] = writing_agents
            logger.info(f"ðŸ“¦ Added {len(writing_agents)} writing agents to response")
        
        if business_intelligence is not None:
            response['business_intelligence'] = business_intelligence
        
        return response
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get processing statistics"""
        return {
            **self.stats,
            'success_rate': (
                self.stats['successful_extractions'] / self.stats['total_extractions']
                if self.stats['total_extractions'] > 0
                else 0
            ),
            'avg_processing_time': (
                self.stats['total_processing_time'] / self.stats['successful_extractions']
                if self.stats['successful_extractions'] > 0
                else 0
            )
        }
    
    async def _run_enhanced_pipeline(
        self,
        raw_extraction: Dict[str, Any],
        file_path: str,
        progress_tracker = None
    ) -> Dict[str, Any]:
        """
        Run the enhanced 3-phase extraction pipeline.
        
        Phase 1: Document Intelligence (already done in raw_extraction)
        Phase 2: Semantic Extraction & Relationship Mapping
        Phase 3: Intelligent Summarization
        
        Args:
            raw_extraction: Results from standard extraction
            file_path: Path to PDF file
            progress_tracker: Optional progress tracking object
            
        Returns:
            Enhanced extraction results with entities, relationships, and intelligent summary
        """
        try:
            # Phase 2: Semantic Extraction & Relationship Mapping
            if progress_tracker:
                await progress_tracker.start_stage(
                    "semantic_analysis",
                    "Mapping entities and relationships..."
                )
            
            logger.info("ðŸ“Š Phase 2: Semantic extraction and relationship mapping...")
            semantic_result = await self.semantic_extractor.extract_entities_and_relationships(
                raw_extraction,
                enhanced_extraction=None
            )
            
            if progress_tracker:
                await progress_tracker.complete_stage(
                    "semantic_analysis",
                    "Entity relationships mapped"
                )
            
            # Phase 3: Intelligent Summarization
            if progress_tracker:
                await progress_tracker.start_stage(
                    "summary_generation",
                    "Generating intelligent summary..."
                )
            
            logger.info("âœï¸ Phase 3: Intelligent summarization...")
            logger.info(f"ðŸ“Š Semantic result keys: {list(semantic_result.keys())}")
            logger.info(f"   - Entities: {'âœ…' if 'entities' in semantic_result else 'âŒ'}")
            logger.info(f"   - Relationships: {'âœ…' if 'relationships' in semantic_result else 'âŒ'}")
            logger.info(f"   - Business Intelligence: {'âœ…' if 'business_intelligence' in semantic_result else 'âŒ'}")
            
            # Import conversational summary service
            from ..conversational_summary_service import ConversationalSummaryService
            summary_service = ConversationalSummaryService()
            
            logger.info(f"ðŸ“ Summary service available: {summary_service.is_available()}")
            
            if summary_service.is_available():
                logger.info("ðŸš€ Calling generate_conversational_summary with use_enhanced=True...")
                summary_result = await summary_service.generate_conversational_summary(
                    extraction_data=semantic_result,
                    document_context={
                        'file_name': os.path.basename(file_path),
                        'extraction_method': 'claude_enhanced'
                    },
                    use_enhanced=True
                )
                logger.info(f"âœ… Summary generation completed. Success: {summary_result.get('success')}")
                if summary_result.get('summary'):
                    logger.info(f"ðŸ“„ Generated summary (first 200 chars): {summary_result.get('summary')[:200]}...")
                
                if progress_tracker:
                    await progress_tracker.complete_stage(
                        "summary_generation",
                        "Intelligent summary generated"
                    )
            else:
                logger.warning("Summary service not available, skipping Phase 3")
                summary_result = {'summary': 'Summary generation not available'}
            
            # Combine all results
            enhanced_result = {
                **raw_extraction,  # Include all original extraction data
                'entities': semantic_result.get('entities', {}),
                'relationships': semantic_result.get('relationships', {}),
                'business_intelligence': semantic_result.get('business_intelligence', {}),
                'summary': summary_result.get('summary', ''),
                'structured_data': summary_result.get('structured_data', {}),  # â† ADD THIS for frontend
                'extraction_pipeline': '3-phase-enhanced',
                'semantic_extraction_success': semantic_result.get('success', False)
            }
            
            logger.info("âœ… Enhanced 3-phase pipeline completed successfully")
            logger.info(f"ðŸ“¦ Enhanced result keys: {list(enhanced_result.keys())}")
            logger.info(f"   - Summary length: {len(enhanced_result.get('summary', ''))} characters")
            logger.info(f"   - Summary preview: {enhanced_result.get('summary', '')[:150]}...")
            logger.info(f"   - Structured data: {enhanced_result.get('structured_data', {})}")
            return enhanced_result
            
        except Exception as e:
            logger.error(f"âŒ Enhanced pipeline failed: {e}")
            # Return original extraction on error
            return {
                **raw_extraction,
                'enhanced_pipeline_error': str(e),
                'extraction_pipeline': 'standard'
            }
    
    async def extract_summarize_data_via_claude(self, file_path: str) -> Dict[str, Any]:
        """
        Extract summarize data using Claude with OCR agent configuration.
        Returns only markdown result as specified.
        """
        start_time = time.time()
        self.stats['total_extractions'] += 1
        
        try:
            logger.info(f"Starting Claude-based summarize extraction for: {file_path}")
            
            # Validate service availability (same as extract_commission_data)
            if not self.is_available():
                raise ValueError("Claude service not available. Check API key and SDK installation.")
            
            # Validate file (same as extract_commission_data)
            validation_result = self._validate_file(file_path)
            if not validation_result['valid']:
                raise ValueError(validation_result['error'])
            
            pdf_info = validation_result['pdf_info']
            
            # Use the same PDF processor as other methods
            pdf_base64 = self.pdf_processor.encode_pdf_to_base64(file_path)
            
            # Use the specific prompt for summarize extraction
            prompt = self.prompts.get_summarize_extraction_prompt()
            
            # Use the same API call method as _extract_standard_file
            logger.info("Calling Claude API for summarize extraction")
            extraction_result = await self._call_claude_api(
                pdf_base64,
                prompt,
                model=self.primary_model,
                pdf_pages=pdf_info.get('page_count', 0),
                use_cache=False
            )
            
            # Extract content from the result
            content = extraction_result.get('content', '')
            logger.info(f"Extracted content length: {len(content) if content else 0}")
            
            # Update statistics (same as extract_commission_data)
            processing_time = time.time() - start_time
            self.stats['successful_extractions'] += 1
            self.stats['total_processing_time'] += processing_time
            
            logger.info(f"âœ… Claude summarize extraction completed in {processing_time:.2f}s")
            
            # Return result in same format as extract_commission_data
            result = {
                'success': True,
                'result': content,  # Only return the markdown result
                'processing_time': processing_time,
                'extraction_method': 'claude',
                'file_info': pdf_info
            }
            logger.info(f"Returning result with keys: {list(result.keys())}")
            return result
                
        except Exception as e:
            self.stats['failed_extractions'] += 1
            processing_time = time.time() - start_time
            
            logger.error(f"âŒ Claude summarize extraction failed after {processing_time:.2f}s: {e}")
            
            return {
                'success': False,
                'error': str(e),
                'error_message': self.error_handler.format_error_message(e),
                'result': '',
                'extraction_method': 'claude',
                'processing_time': processing_time
            }

